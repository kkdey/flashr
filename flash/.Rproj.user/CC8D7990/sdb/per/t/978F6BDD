{
    "contents" : "---\ntitle: \"backfitreport\"\noutput: pdf_document\n---\n\n```{r,echo=FALSE,message=FALSE}\nlibrary(\"gtools\")\nlibrary(\"MASS\")\nlibrary(\"PMA\")\nlibrary(\"ashr\")\nsource('~/HG/ash-sfa/Rcode/postmean/flash/flash_VEM.R')\nsource('~/HG/ash-sfa/Rcode/postmean/flash/backfitting.R')\nsource('~/HG/ash-sfa/Rcode/postmean/flash/flash_v_K.R')\nsim_K = function(K, N, P, SF, SL, signal,noise){\n  E = matrix(rnorm(N*P,0,noise),nrow=N)\n  Y = E\n  L_true = array(0, dim = c(N,K))\n  F_true = array(0, dim = c(P,K))\n  \n  for(k in 1:K){\n    lstart = rnorm(N, 0, signal)\n    fstart = rnorm(P, 0, signal)\n    \n    index = sample(seq(1:N),(N*SL))\n    lstart[index] = 0\n    index = sample(seq(1:P),(P*SF))\n    fstart[index] = 0\n    \n    L_true[,k] = lstart\n    F_true[,k] = fstart\n    \n    Y = Y + lstart %*% t(fstart)\n  }\n  return(list(Y = Y, L_true = L_true, F_true = F_true, Error = E))\n}\n```\n\n```{r,cache=TRUE,eval=TRUE}\ndata = sim_K(K=5,N=60, P=100, SF = 0.5, SL = 0.5, signal = 5,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 5\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=20)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\ndim(gl)\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\ngback = backfitting(Y,gl,gf)\ngbl = gback$Lest\ngbf = gback$Fest\ndim(gbl)\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\n\n# greedy backfit\nN = dim(Y)[1]\nP = dim(Y)[2]\nK = 20\n#initial the residual for the greedy algorithm\nresidual = Y\n# do rank one decomposition\nr_flash = flash_VEM(residual)\nl_temp = r_flash$l\nf_temp = r_flash$f\n# test whether it is zero\nif(sum(l_temp^2)==0 | sum(f_temp^2)==0){\n  L_out = 0\n  F_out = 0\n  return(list(l = L_out,f = F_out))\n}else{\n  L_out = l_temp\n  F_out = f_temp\n  #get the new residual\n  # residual = Y - L_out %*% t(F_out)\n  #itreation for the rank K-1 \n  for(k in 1:K){\n    #rank one for residual \n    residual = Y - L_out %*% t(F_out)\n    r_flash = flash_VEM(residual)\n    l_temp = r_flash$l\n    f_temp = r_flash$f\n    # get the new residual\n    # residual = residual - l_temp %*% t(f_temp)\n    #check if we should stop at this iteration\n    if(sum(l_temp^2)==0 | sum(f_temp^2)==0){\n      break\n      print(k)\n    }else{\n      # if not go to next step and restore the l and f\n      L_out = cbind(L_out,l_temp)\n      F_out = cbind(F_out,f_temp)\n    }\n    gback = backfitting(Y,L_out,F_out)\n    L_out = gback$Lest\n    F_out = gback$Fest\n    # for loop can control the number of the factors if needed\n  }\n}\ndim(L_out)\nsqrt(mean(((Y  - L_out%*%t(F_out) )- E)^2))\ngback = backfitting(Y,svdl[,1:svdK],svdf[,1:svdK]%*% diag(svdd[1:svdK]))\ngbl = gback$Lest\ngbf = gback$Fest\ndim(gbl)\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\n\n```\n\nThat is the final round about this solution, and I will start from the beginning and explain this procedure step by step. \n\\newpage\n\\section{Greedy Algorithm}\nFirst we would say that the idea of backfitting does work here, but backfitting need initial value. \n\nWe first start from the greedy algorithm and see the performance. We start from the simple case.\n\n```{r,cache=TRUE,eval= TRUE}\ndata = sim_K(K=2,N=60, P=100, SF = 0.5, SL = 0.5, signal = 1,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 2\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=20)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\ndim(gl)\n```\n\nHere we can see that the greedy algorithm has already give us reasonably good result and the correct rank. \n\nlet's see what backfitting can contribute here\n\n```{r,cache=TRUE,eval=TRUE}\ngback = backfitting(Y,gl,gf)\ngbl = gback$Lest\ngbf = gback$Fest\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\ndim(gbl)\n```\n\nSo here we notice that we can improve by backfitting.\n\nIf we change the signal \n\n```{r,cache=TRUE,eval= TRUE}\ndata = sim_K(K=2,N=60, P=100, SF = 0.5, SL = 0.5, signal = 0.5,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 2\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=20)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\nsum(gl^2)\nsum(gf^2)\nsqrt(mean(((Y  - gl*gf )- E)^2))\n\n```\n\nWe want the structure is 0. We don't need backfitting. \n\nHow about bigger one?\n\n```{r,cache=TRUE,eval= TRUE}\ndata = sim_K(K=2,N=60, P=100, SF = 0.5, SL = 0.5, signal = 0.8,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 2\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=20)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\ndim(gl)\ngback = backfitting(Y,gl,gf)\ngbl = gback$Lest\ngbf = gback$Fest\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\ndim(gbl)\n```\n\nCan we find some case that SVD can not find the true rank easily?\n\n```{r,cache=TRUE,eval= TRUE}\ndata = sim_K(K=2,N=60, P=100, SF = 0.5, SL = 0.5, signal = 0.6,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 2\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=20)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\ndim(gl)\ngback = backfitting(Y,gl,gf)\ngbl = gback$Lest\ngbf = gback$Fest\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\ndim(gbl)\n```\n\nWhat if the rank is slightly bigger?\n\n```{r,cache=TRUE,eval= TRUE}\ndata = sim_K(K=3,N=60, P=100, SF = 0.5, SL = 0.5, signal = 0.6,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 3\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=20)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\ndim(gl)\ngback = backfitting(Y,gl,gf)\ngbl = gback$Lest\ngbf = gback$Fest\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\ndim(gbl)\n```\n\nHow about we have many small factors, shall we pick all of them or throw them? \n\n```{r,cache=TRUE,eval= TRUE}\ndata = sim_K(K=10,N=60, P=100, SF = 0.5, SL = 0.5, signal = 0.5,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 10\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=20)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\nis.vector(gl)\nsqrt(mean(((Y  - 0 )- E)^2))\n```\n\nThrowing all factors or adding them all might not a good idea. We can see that here we use rank one from the greedy algorithm. \n\nWhat are those initial values from greedy algorithm? Is that would be a good initial value? Can we use the SVD as initial value (suppose we know the truth)? What would that be like?\n\n\n```{r,cache=TRUE,eval= TRUE}\ndata = sim_K(K=15,N=60, P=100, SF = 0.5, SL = 0.5, signal = 0.6,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 15\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=30)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\ndim(gl)\ngback = backfitting(Y,gl,gf)\ngbl = gback$Lest\ngbf = gback$Fest\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\ndim(gbl)\n\ngback = backfitting(Y,svdl[,1:svdK],svdf[,1:svdK]%*% diag(svdd[1:svdK]) )\ngbl = gback$Lest\ngbf = gback$Fest\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\ndim(gbl)\n```\n\nWe know two things so far. One is that greedy algorithm give a smaller rank, and another is that backfitting can also provide smaller rank reasonably.\n\nWe want to check the fact that backfitting can remove the factors.\n\n```{r,cache=TRUE,eval= TRUE}\ndata = sim_K(K=3,N=60, P=100, SF = 0.5, SL = 0.5, signal = 0.5,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 3\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \n\n\ngback = backfitting(Y,svdl[,1:svdK],svdf[,1:svdK]%*% diag(svdd[1:svdK]) )\ngbl = gback$Lest\ngbf = gback$Fest\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\nsum(gbl^2)\nsum(gbf^2)\n```\n\nStarting from SVD initial values, backfitting can also give a reasonable rank. We should keep this fact in mind that backfitting can remove some unwanted structure.\n\nThere is a problem that when the signal is too big, SVD should be good enough since the noise is so small that we can ignore that. If we propose greedy algorithm on this model or data, the rank can not be close to the truth. In the first step, the model is not really a rank one model, so the estimation might not be that good. There might be something remain due to the sparsity of the structure estimation. It tend to be pick larger rank.\n\n```{r,cache=TRUE,eval= TRUE}\ndata = sim_K(K=2,N=60, P=100, SF = 0.5, SL = 0.5, signal = 3,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 2\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=20)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\ndim(gl)\ngback = backfitting(Y,gl,gf)\ngbl = gback$Lest\ngbf = gback$Fest\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\ndim(gbl)\n```\n\nBy doing backfitting, we can get better estimation not only for the structure but also for the rank.\n\nSo far so good. However, when the rank is larger, the problem is more obvious.\n\n```{r,cache=TRUE,eval= TRUE}\ndata = sim_K(K=3,N=60, P=100, SF = 0.5, SL = 0.5, signal = 3,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 3\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=20)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\ndim(gl)\ngback = backfitting(Y,gl,gf)\ngbl = gback$Lest\ngbf = gback$Fest\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\ndim(gbl)\n```\n\nWe can see that backfitting helps a little for the structure estimation but not for the rank estimation. Is the statement I said above right? Let's try smaller signal.\n\n```{r,cache=TRUE,eval= TRUE}\ndata = sim_K(K=3,N=60, P=100, SF = 0.5, SL = 0.5, signal = 1,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 3\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=20)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\ndim(gl)\ngback = backfitting(Y,gl,gf)\ngbl = gback$Lest\ngbf = gback$Fest\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\ndim(gbl)\n```\n\nNow we pick the right rank. \n\nSo the problem is that greedy algorithm tends to add too much in the procedure in some situations. Backfitting can not help to remove the unwanted structure.\n\n\\newpage\n\\section{GreedyBack}\nWe propose a greedy backfitting algorithm which is useful but time consuming. See the page 1.\n\nHow about the smaller signal case which might be more interesting?\n\n```{r,cache=TRUE,eval=TRUE}\ndata = sim_K(K=15,N=60, P=100, SF = 0.5, SL = 0.5, signal = 0.6,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 15\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n# try greedy algorithm \nggreedy = flash_v_K(Y,K=30)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\ndim(gl)\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\ngback = backfitting(Y,gl,gf)\ngbl = gback$Lest\ngbf = gback$Fest\ndim(gbl)\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\n\n# greedy backfit\nN = dim(Y)[1]\nP = dim(Y)[2]\nK = 30\n#initial the residual for the greedy algorithm\nresidual = Y\n# do rank one decomposition\nr_flash = flash_VEM(residual)\nl_temp = r_flash$l\nf_temp = r_flash$f\n# test whether it is zero\nif(sum(l_temp^2)==0 | sum(f_temp^2)==0){\n  L_out = 0\n  F_out = 0\n  return(list(l = L_out,f = F_out))\n}else{\n  L_out = l_temp\n  F_out = f_temp\n  #get the new residual\n  # residual = Y - L_out %*% t(F_out)\n  #itreation for the rank K-1 \n  for(k in 1:K){\n    #rank one for residual \n    residual = Y - L_out %*% t(F_out)\n    r_flash = flash_VEM(residual)\n    l_temp = r_flash$l\n    f_temp = r_flash$f\n    # get the new residual\n    # residual = residual - l_temp %*% t(f_temp)\n    #check if we should stop at this iteration\n    if(sum(l_temp^2)==0 | sum(f_temp^2)==0){\n      break\n      print(k)\n    }else{\n      # if not go to next step and restore the l and f\n      L_out = cbind(L_out,l_temp)\n      F_out = cbind(F_out,f_temp)\n    }\n    gback = backfitting(Y,L_out,F_out)\n    L_out = gback$Lest\n    F_out = gback$Fest\n    # for loop can control the number of the factors if needed\n  }\n}\ndim(L_out)\nsqrt(mean(((Y  - L_out%*%t(F_out) )- E)^2))\ngback = backfitting(Y,svdl[,1:svdK],svdf[,1:svdK]%*% diag(svdd[1:svdK]))\ngbl = gback$Lest\ngbf = gback$Fest\ndim(gbl)\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\n\n```\n\nwe can see that it is better than greedy + backfitting. We also show the result under the assumption that we know the true rank is 15. \n\n\\newpage\n\\section{Comparison}\n```{r,cache=TRUE,eval=TRUE}\ndata = sim_K(K=5,N=60, P=100, SF = 0.5, SL = 0.5, signal = 0.6,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 5\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n\nout = PMD(Y, type=\"standard\", K=5, sumabs=0.5)\nsqrt(mean(((Y  - out$u%*% diag(out$d) %*% t(out$v) )- E)^2))\n\nggreedy = flash_v_K(Y,K=20)\n# initial gl and gf as svd\ngl = ggreedy$l\ngf = ggreedy$f\nsqrt(mean(((Y  - gl%*%t(gf) )- E)^2))\ngback = backfitting(Y,gl,gf)\ngbl = gback$Lest\ngbf = gback$Fest\nsqrt(mean(((Y  - gbl%*%t(gbf) )- E)^2))\n```\n\nPMD is reasonalbly good if we know the truth.\n\n```{r,cache=TRUE,eval=TRUE}\nsource('~/HG/ash-sfa/Rcode/postmean/flash/greedyback.R')\ndata = sim_K(K=5,N=60, P=100, SF = 0.5, SL = 0.5, signal = 5,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 5\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n\nout = PMD(Y, type=\"standard\", K=5, sumabs=0.5)\nsqrt(mean(((Y  - out$u%*% diag(out$d) %*% t(out$v) )- E)^2))\n\nggrdbk = Greedyback(Y,K=20,tol=1e-4,tau = 20)\nsqrt(mean(((Y  - ggrdbk$l%*%t(ggrdbk$f) )- E)^2))\ndim(ggrdbk$l)\n```\n\nFor large signal, we just want to be as good as SVD.\n\nLet me show one more example that good enough.\n```{r,cache=TRUE,eval=TRUE}\nsource('~/HG/ash-sfa/Rcode/postmean/flash/greedyback.R')\ndata = sim_K(K=2,N=60, P=100, SF = 0.5, SL = 0.5, signal = 5,noise = 1)\nY = data$Y\nL_true = data$L_true\nF_true = data$F_true\nE = data$Error\nsvdl = svd(Y)$u\nsvdf = svd(Y)$v\nsvdd = svd(Y)$d\nplot(svdd)\nsvdK = 2\nsqrt(mean(((Y  - svdl[,1:svdK]%*% diag(svdd[1:svdK]) %*% t(svdf[,1:svdK]) )- E)^2))\n\nout = PMD(Y, type=\"standard\", K=2, sumabs=0.5)\nsqrt(mean(((Y  - out$u%*% diag(out$d) %*% t(out$v) )- E)^2))\n\nggrdbk = Greedyback(Y,K=20,tol=1e-4,tau = 20)\nsqrt(mean(((Y  - ggrdbk$l%*%t(ggrdbk$f) )- E)^2))\ndim(ggrdbk$l)\n```\n\n```{r,cache=TRUE,eval=TRUE,message=FALSE}\nfor(i in 2:10){\n  lambda = i/10 \n  out = PMD(Y, type=\"standard\", K=2, sumabs=lambda)\n  print(sqrt(mean(((Y  - out$u%*% diag(out$d) %*% t(out$v) )- E)^2)))\n}\n\n```",
    "created" : 1455155674958.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3912356686",
    "id" : "978F6BDD",
    "lastKnownWriteTime" : 1454963719,
    "path" : "~/HG/ash-sfa/Rcode/postmean/flash/simulation/backfitreport.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "type" : "r_markdown"
}